AWS S3操作手册

(一)	配置S3 Buckets
访问：https://console.aws.amazon.com/s3/home进入AWS S3的管理界面：
 
点击 按钮后，输入一个合规的名称并选择一个Region，创建S3 Bucket。
 
生成的S3 Bucket访问地址（Endpoint）可以从Manage Console右侧的“properties”页面中的“Static Website Hosting”中获取。
 

(二)	获取S3 Buckets Access Key
要通过Tools访问S3时，需要提供Access Key和Secret Key，获取方法为：
访问https://console.aws.amazon.com/iam/home?#security_credential网站，点击 按钮生成Access Key，但是注意可同时维护的Key的数量。	
 
从生成Key的页面中，注意保存Access Key ID和Secret Access Key，并下载一份保存在本地。
 
(三)	使用S3 Buckets
1)	通过静态页面访问
用户可以在 Amazon S3 上托管静态网站。在静态网站上，单独的网页包含静态内容。它们也可能包含客户端脚本。通过对比得知，动态网站依赖服务器端处理，包括诸如 PHP、JSP 或 ASP.NET 的服务器端脚本。Amazon S3 不支持服务器端脚本编写。
Note
Amazon Web Services (AWS) 提供用于托管动态网站的资源。若要了解有关 AWS 上的网站托管的更多信息，请转到网站和网站托管。
若要托管静态网站，您需要为网站托管配置 Amazon S3 存储桶，然后将网站内容上传到存储桶。然后即可在存储桶的特定区域网站终端节点上使用该网站：
<bucket-name>.s3-website-<AWS-region>.amazonaws.com
有关 Amazon S3 的特定区域网站终端节点列表，请参阅网站终端节点。例如，假设您在美国标准区域创建了名为 examplebucket 的存储桶，并将其配置为网站。 以下示例 URL 将提供对您的网站内容的访问：
•	此 URL 将返回您为该网站配置的默认索引文档。
http://examplebucket.s3-website-us-east-1.amazonaws.com/
•	此 URL 将请求 photo.jpg 数据元，该数据元存储在存储桶的根级。
http://examplebucket.s3-website-us-east-1.amazonaws.com/photo.jpg
•	此 URL 将请求存储桶中的 docs/doc1.html 数据元。
http://examplebucket.s3-website-us-east-1.amazonaws.com/docs/doc1.html

2)	通过EC2文件系统方式访问
在EC2中通过file system方式使用S3，需要用到一个工具：s3fs，该工具可从以下站点获取
https://github.com/s3fs-fuse/s3fs-fuse
以下以Ubuntu操作系统为例，其他操作系统的使用方式，转换成对应系统的命令即可。
# apt-get update
# apt-get install build-essential libcurl4-openssl-dev libxml2-dev mime-support
# apt-get install git libfuse-dev automake libtoollibnet-amazon-s3-tools-perl
# apt-get install pkg-config libssl-dev
# s3ls --access-key <AWS_ACCESS_KEY_ID> --secret-key <AWS_ACCESS_KEY_SECRET>
//**通过s3ls命令可以查看S3 Buckets中的可用目录
# echo <AWS_ACCESS_KEY_ID>:<AWS_ACCESS_KEY_SECRET> > ~/.passwd-s3fs
//**将S3 Buckets访问密钥存储在系统中，以便于s3fs调用
# git clone https://github.com/s3fs-fuse/s3fs-fuse
# cd s3fs-fue
# ls -l
total 84
-rw-r--r-- 1 root root   414 Dec 23 03:29 AUTHORS
-rwxr-xr-x 1 root root  1225 Dec 23 03:29 autogen.sh
-rw-r--r-- 1 root root  8487 Dec 23 03:29 ChangeLog
-rw-r--r-- 1 root root  8147 Dec 23 03:29 configure.ac
-rw-r--r-- 1 root root 17987 Dec 23 03:29 COPYING
drwxr-xr-x 3 root root  4096 Dec 23 03:29 doc
-rw-r--r-- 1 root root 15578 Dec 23 03:29 INSTALL
-rw-r--r-- 1 root root  1520 Dec 23 03:29 Makefile.am
-rw-r--r-- 1 root root  3286 Dec 23 03:29 README.md
drwxr-xr-x 2 root root  4096 Dec 23 03:29 src
drwxr-xr-x 2 root root  4096 Dec 23 03:29 test
# ./autogen.sh
--- Make commit hash file -------
--- Finished commit hash file ---
--- Start autotools -------------
configure.ac:30: installing './compile'
configure.ac:26: installing './config.guess'
configure.ac:26: installing './config.sub'
configure.ac:27: installing './install-sh'
configure.ac:27: installing './missing'
src/Makefile.am: installing './depcomp'
parallel-tests: installing './test-driver'
--- Finished autotools ----------
# ./configure --prefix=/usr --with-openssl
checking build system type... x86_64-unknown-linux-gnu
checking host system type... x86_64-unknown-linux-gnu
checking target system type... x86_64-unknown-linux-gnu
checking for a BSD-compatible install... /usr/bin/install -c
checking whether build environment is sane... yes
checking for a thread-safe mkdir -p... /bin/mkdir -p
checking for gawk... gawk
checking whether make sets $(MAKE)... yes
checking whether make supports nested variables... yes
checking for g++... g++
checking whether the C++ compiler works... yes
checking for C++ compiler default output file name... a.out
checking for suffix of executables...
checking whether we are cross compiling... no
checking for suffix of object files... o
checking whether we are using the GNU C++ compiler... yes
checking whether g++ accepts -g... yes
checking for style of include used by make... GNU
checking dependency style of g++... gcc3
checking for gcc... gcc
checking whether we are using the GNU C compiler... yes
checking whether gcc accepts -g... yes
checking for gcc option to accept ISO C89... none needed
checking whether gcc understands -c and -o together... yes
checking dependency style of gcc... gcc3
checking s3fs build with nettle(GnuTLS)... no
checking s3fs build with OpenSSL... yes
checking s3fs build with GnuTLS... no
checking s3fs build with NSS... no
checking for pkg-config... /usr/bin/pkg-config
checking pkg-config is at least version 0.9.0... yes
checking for common_lib_checking... yes
checking compile s3fs with... OpenSSL
checking for DEPS... yes
checking for malloc_trim... yes
checking pthread mutex recursive... PTHREAD_MUTEX_RECURSIVE
checking for git... yes
checking for .git... yes
checking github short commit hash... ea151a7
checking that generated files are newer than configure... done
configure: creating ./config.status
config.status: creating Makefile
config.status: creating src/Makefile
config.status: creating test/Makefile
config.status: creating doc/Makefile
config.status: creating config.h
config.status: executing depfiles commands
#
# make
make  all-recursive
make[1]: Entering directory `/root/s3fs-fuse'
Making all in src
make[2]: Entering directory `/root/s3fs-fuse/src'
g++ -DHAVE_CONFIG_H -I. -I..  -D_FILE_OFFSET_BITS=64 -I/usr/include/fuse -I/usr/include/libxml2      -g -O2 -Wall -D_FILE_OFFSET_BITS=64 -MT s3fs.o -MD -MP -MF .deps/s3fs.Tpo -c -o s3fs.o s3fs.cpp
mv -f .deps/s3fs.Tpo .deps/s3fs.Po
g++ -DHAVE_CONFIG_H -I. -I..  -D_FILE_OFFSET_BITS=64 -I/usr/include/fuse -I/usr/include/libxml2      -g -O2 -Wall -D_FILE_OFFSET_BITS=64 -MT curl.o -MD -MP -MF .deps/curl.Tpo -c -o curl.o curl.cpp
mv -f .deps/curl.Tpo .deps/curl.Po
g++ -DHAVE_CONFIG_H -I. -I..  -D_FILE_OFFSET_BITS=64 -I/usr/include/fuse -I/usr/include/libxml2      -g -O2 -Wall -D_FILE_OFFSET_BITS=64 -MT cache.o -MD -MP -MF .deps/cache.Tpo -c -o cache.o cache.cpp
mv -f .deps/cache.Tpo .deps/cache.Po
g++ -DHAVE_CONFIG_H -I. -I..  -D_FILE_OFFSET_BITS=64 -I/usr/include/fuse -I/usr/include/libxml2      -g -O2 -Wall -D_FILE_OFFSET_BITS=64 -MT string_util.o -MD -MP -MF .deps/string_util.Tpo -c -o string_util.o string_util.cpp
mv -f .deps/string_util.Tpo .deps/string_util.Po
g++ -DHAVE_CONFIG_H -I. -I..  -D_FILE_OFFSET_BITS=64 -I/usr/include/fuse -I/usr/include/libxml2      -g -O2 -Wall -D_FILE_OFFSET_BITS=64 -MT s3fs_util.o -MD -MP -MF .deps/s3fs_util.Tpo -c -o s3fs_util.o s3fs_util.cpp
mv -f .deps/s3fs_util.Tpo .deps/s3fs_util.Po
g++ -DHAVE_CONFIG_H -I. -I..  -D_FILE_OFFSET_BITS=64 -I/usr/include/fuse -I/usr/include/libxml2      -g -O2 -Wall -D_FILE_OFFSET_BITS=64 -MT fdcache.o -MD -MP -MF .deps/fdcache.Tpo -c -o fdcache.o fdcache.cpp
mv -f .deps/fdcache.Tpo .deps/fdcache.Po
g++ -DHAVE_CONFIG_H -I. -I..  -D_FILE_OFFSET_BITS=64 -I/usr/include/fuse -I/usr/include/libxml2      -g -O2 -Wall -D_FILE_OFFSET_BITS=64 -MT common_auth.o -MD -MP -MF .deps/common_auth.Tpo -c -o common_auth.o common_auth.cpp
mv -f .deps/common_auth.Tpo .deps/common_auth.Po
g++ -DHAVE_CONFIG_H -I. -I..  -D_FILE_OFFSET_BITS=64 -I/usr/include/fuse -I/usr/include/libxml2      -g -O2 -Wall -D_FILE_OFFSET_BITS=64 -MT openssl_auth.o -MD -MP -MF .deps/openssl_auth.Tpo -c -o openssl_auth.o openssl_auth.cpp
mv -f .deps/openssl_auth.Tpo .deps/openssl_auth.Po
g++  -g -O2 -Wall -D_FILE_OFFSET_BITS=64   -o s3fs s3fs.o curl.o cache.o string_util.o s3fs_util.o fdcache.o common_auth.o openssl_auth.o   -pthread -lfuse -lcurl -lxml2 -lcrypto
g++ -DHAVE_CONFIG_H -I. -I..  -D_FILE_OFFSET_BITS=64 -I/usr/include/fuse -I/usr/include/libxml2      -g -O2 -Wall -D_FILE_OFFSET_BITS=64 -MT test_string_util.o -MD -MP -MF .deps/test_string_util.Tpo -c -o test_string_util.o test_string_util.cpp
mv -f .deps/test_string_util.Tpo .deps/test_string_util.Po
g++  -g -O2 -Wall -D_FILE_OFFSET_BITS=64   -o test_string_util string_util.o test_string_util.o
make[2]: Leaving directory `/root/s3fs-fuse/src'
Making all in test
make[2]: Entering directory `/root/s3fs-fuse/test'
make[2]: Nothing to be done for `all'.
make[2]: Leaving directory `/root/s3fs-fuse/test'
Making all in doc
make[2]: Entering directory `/root/s3fs-fuse/doc'
make[2]: Nothing to be done for `all'.
make[2]: Leaving directory `/root/s3fs-fuse/doc'
make[2]: Entering directory `/root/s3fs-fuse'
make[2]: Leaving directory `/root/s3fs-fuse'
make[1]: Leaving directory `/root/s3fs-fuse'
# make install
Making install in src
make[1]: Entering directory `/root/s3fs-fuse/src'
make[2]: Entering directory `/root/s3fs-fuse/src'
 /bin/mkdir -p '/usr/bin'
  /usr/bin/install -c s3fs '/usr/bin'
make[2]: Nothing to be done for `install-data-am'.
make[2]: Leaving directory `/root/s3fs-fuse/src'
make[1]: Leaving directory `/root/s3fs-fuse/src'
Making install in test
make[1]: Entering directory `/root/s3fs-fuse/test'
make[2]: Entering directory `/root/s3fs-fuse/test'
make[2]: Nothing to be done for `install-exec-am'.
make[2]: Nothing to be done for `install-data-am'.
make[2]: Leaving directory `/root/s3fs-fuse/test'
make[1]: Leaving directory `/root/s3fs-fuse/test'
Making install in doc
make[1]: Entering directory `/root/s3fs-fuse/doc'
make[2]: Entering directory `/root/s3fs-fuse/doc'
make[2]: Nothing to be done for `install-exec-am'.
 /bin/mkdir -p '/usr/share/man/man1'
 /usr/bin/install -c -m 644 man/s3fs.1 '/usr/share/man/man1'
make[2]: Leaving directory `/root/s3fs-fuse/doc'
make[1]: Leaving directory `/root/s3fs-fuse/doc'
make[1]: Entering directory `/root/s3fs-fuse'
make[2]: Entering directory `/root/s3fs-fuse'
make[2]: Nothing to be done for `install-exec-am'.
make[2]: Nothing to be done for `install-data-am'.
make[2]: Leaving directory `/root/s3fs-fuse'
make[1]: Leaving directory `/root/s3fs-fuse'
# which s3fs
/usr/bin/s3fs
# s3fs -h
Usage: s3fs BUCKET:[PATH] MOUNTPOINT [OPTION]...

Mount an Amazon S3 bucket as a file system.

   General forms for s3fs and FUSE/mount options:
      -o opt[,opt...]
      -o opt [-o opt] ...

s3fs Options:

   Most s3fs options are given in the form where "opt" is:

             <option_name>=<option_value>

   default_acl (default="private")
     - the default canned acl to apply to all written s3 objects
          see http://aws.amazon.com/documentation/s3/ for the
          full list of canned acls

   retries (default="2")
      - number of times to retry a failed s3 transaction

   use_cache (default="" which means disabled)
      - local folder to use for local file cache

   del_cache (delete local file cache)
      - delete local file cache when s3fs starts and exits.

   storage_class (default="standard")
      - store object with specified storage class.  Possible values:
        standard, standard_ia, and reduced_redundancy.

   use_sse (default is disable)
      - Specify three type Amazon's Server-Site Encryption: SSE-S3,
        SSE-C or SSE-KMS. SSE-S3 uses Amazon S3-managed encryption
        keys, SSE-C uses customer-provided encryption keys, and
        SSE-KMS uses the master key which you manage in AWS KMS.
        You can specify "use_sse" or "use_sse=1" enables SSE-S3
        type(use_sse=1 is old type parameter).
        Case of setting SSE-C, you can specify "use_sse=custom",
        "use_sse=custom:<custom key file path>" or
        "use_sse=<custom key file path>"(only <custom key file path>
        specified is old type parameter). You can use "c" for
        short "custom".
        The custom key file must be 600 permission. The file can
        have some lines, each line is one SSE-C key. The first line
        in file is used as Customer-Provided Encryption Keys for
        uploading and changing headers etc. If there are some keys
        after first line, those are used downloading object which
        are encrypted by not first key. So that, you can keep all
        SSE-C keys in file, that is SSE-C key history.
        If you specify "custom"("c") without file path, you
        need to set custom key by load_sse_c option or AWSSSECKEYS
        environment.(AWSSSECKEYS environment has some SSE-C keys
        with ":" separator.) This option is used to decide the
        SSE type. So that if you do not want to encrypt a object
        object at uploading, but you need to decrypt encrypted
        object at downloaing, you can use load_sse_c option instead
        of this option.
        For setting SSE-KMS, specify "use_sse=kmsid" or
        "use_sse=kmsid:<kms id>". You can use "k" for short "kmsid".
        If you san specify SSE-KMS type with your <kms id> in AWS
        KMS, you can set it after "kmsid:"(or "k:"). If you
        specify only "kmsid"("k"), you need to set AWSSSEKMSID
        environment which value is <kms id>. You must be careful
        about that you can not use the KMS id which is not same EC2
        region.

   load_sse_c - specify SSE-C keys
        Specify the custom-provided encription keys file path for decrypting
        at duwnloading.
        If you use the custom-provided encription key at uploading, you
        specify with "use_sse=custom". The file has many lines, one line
        means one custom key. So that you can keep all SSE-C keys in file,
        that is SSE-C key history. AWSSSECKEYS environment is as same as this
        file contents.

   public_bucket (default="" which means disabled)
      - anonymously mount a public bucket when set to 1

   passwd_file (default="")
      - specify which s3fs password file to use

   ahbe_conf (default="" which means disabled)
      - This option specifies the configuration file path which
      file is the additional HTTP header by file(object) extension.
      The configuration file format is below:
      -----------
      line         = [file suffix] HTTP-header [HTTP-values]
      file suffix  = file(object) suffix, if this field is empty,
                     it means "*"(all object).
      HTTP-header  = additional HTTP header name
      HTTP-values  = additional HTTP header value
      -----------
      Sample:
      -----------
      .gz      Content-Encoding     gzip
      .Z       Content-Encoding     compress
               X-S3FS-MYHTTPHEAD    myvalue
      -----------
      A sample configuration file is uploaded in "test" directory.
      If you specify this option for set "Content-Encoding" HTTP
      header, please take care for RFC 2616.

   connect_timeout (default="300" seconds)
      - time to wait for connection before giving up

   readwrite_timeout (default="60" seconds)
      - time to wait between read/write activity before giving up

   max_stat_cache_size (default="1000" entries (about 4MB))
      - maximum number of entries in the stat cache

   stat_cache_expire (default is no expire)
      - specify expire time(seconds) for entries in the stat cache.

   enable_noobj_cache (default is disable)
      - enable cache entries for the object which does not exist.
      s3fs always has to check whether file(or sub directory) exists
      under object(path) when s3fs does some command, since s3fs has
      recognized a directory which does not exist and has files or
      sub directories under itself. It increases ListBucket request
      and makes performance bad.
      You can specify this option for performance, s3fs memorizes
      in stat cache that the object(file or directory) does not exist.

   no_check_certificate
      - server certificate won't be checked against the available
      certificate authorities.

   nodnscache (disable dns cache)
      - s3fs is always using dns cache, this option make dns cache disable.

   nosscache (disable ssl session cache)
      - s3fs is always using ssl session cache, this option make ssl
      session cache disable.

   multireq_max (default="20")
      - maximum number of parallel request for listing objects.

   parallel_count (default="5")
      - number of parallel request for uploading big objects.
      s3fs uploads large object(over 20MB) by multipart post request,
      and sends parallel requests.
      This option limits parallel request count which s3fs requests
      at once. It is necessary to set this value depending on a CPU
      and a network band.

   multipart_size (default="10")
      - part size, in MB, for each multipart request.

   ensure_diskfree (default same multipart_size value)
      - sets MB to ensure disk free space. s3fs makes file for
        downloading, uploading and caching files. If the disk free
        space is smaller than this value, s3fs do not use diskspace
        as possible in exchange for the performance.

   singlepart_copy_limit (default="5120")
      - maximum size, in MB, of a single-part copy before trying
      multipart copy.

   url (default="http://s3.amazonaws.com")
      - sets the url to use to access amazon s3

   endpoint (default="us-east-1")
      - sets the endpoint to use on signature version 4
      If this option is not specified, s3fs uses "us-east-1" region as
      the default. If the s3fs could not connect to the region specified
      by this option, s3fs could not run. But if you do not specify this
      option, and if you can not connect with the default region, s3fs
      will retry to automatically connect to the other region. So s3fs
      can know the correct region name, because s3fs can find it in an
      error from the S3 server.

   sigv2 (default is signature version 4)
      - sets signing AWS requests by sing Signature Version 2

   mp_umask (default is "0000")
      - sets umask for the mount point directory.
      If allow_other option is not set, s3fs allows access to the mount
      point only to the owner. In the opposite case s3fs allows access
      to all users as the default. But if you set the allow_other with
      this option, you can control the permissions of the
      mount point by this option like umask.

   nomultipart (disable multipart uploads)

   enable_content_md5 (default is disable)
      - ensure data integrity during writes with MD5 hash.

   iam_role (default is no role)
      - set the IAM Role that will supply the credentials from the
      instance meta-data.

   noxmlns (disable registering xml name space)
        disable registering xml name space for response of
        ListBucketResult and ListVersionsResult etc. Default name
        space is looked up from "http://s3.amazonaws.com/doc/2006-03-01".
        This option should not be specified now, because s3fs looks up
        xmlns automatically after v1.66.

   nocopyapi (for other incomplete compatibility object storage)
        For a distributed object storage which is compatibility S3
        API without PUT(copy api).
        If you set this option, s3fs do not use PUT with
        "x-amz-copy-source"(copy api). Because traffic is increased
        2-3 times by this option, we do not recommend this.

   norenameapi (for other incomplete compatibility object storage)
        For a distributed object storage which is compatibility S3
        API without PUT(copy api).
        This option is a subset of nocopyapi option. The nocopyapi
        option does not use copy-api for all command(ex. chmod, chown,
        touch, mv, etc), but this option does not use copy-api for
        only rename command(ex. mv). If this option is specified with
        nocopyapi, then s3fs ignores it.

   use_path_request_style (use legacy API calling style)
        Enble compatibility with S3-like APIs which do not support
        the virtual-host request style, by using the older path request
        style.

   dbglevel (default="crit")
        Set the debug message level. set value as crit(critical), err
        (error), warn(warning), info(information) to debug level.
        default debug level is critical. If s3fs run with "-d" option,
        the debug level is set information. When s3fs catch the signal
        SIGUSR2, the debug level is bumpup.

   curldbg - put curl debug message
        Put the debug message from libcurl when this option is specified.

FUSE/mount Options:

   Most of the generic mount options described in 'man mount' are
   supported (ro, rw, suid, nosuid, dev, nodev, exec, noexec, atime,
   noatime, sync async, dirsync).  Filesystems are mounted with
   '-onodev,nosuid' by default, which can only be overridden by a
   privileged user.

   There are many FUSE specific mount options that can be specified.
   e.g. allow_other  See the FUSE's README for the full set.

Miscellaneous Options:

 -h, --help        Output this help.
     --version     Output version info.
 -d  --debug       Turn on DEBUG messages to syslog. Specifying -d
                   twice turns on FUSE debug messages to STDOUT.
 -f                FUSE foreground option - do not run as daemon.
 -s                FUSE singlethread option
                   disable multi-threaded operation


s3fs home page: <https://github.com/s3fs-fuse/s3fs-fuse>
#
# s3fs -o use_cache=/tmp filestorage /s3
# df
Filesystem        1K-blocks    Used    Available Use% Mounted on
/dev/xvda1          8115168 2076536      5603356  28% /
none                      4       0            4   0% /sys/fs/cgroup
udev                 503188      12       503176   1% /dev
tmpfs                101632     336       101296   1% /run
none                   5120       0         5120   0% /run/lock
none                 508144       0       508144   0% /run/shm
none                 102400       0       102400   0% /run/user
s3fs           274877906944       0 274877906944   0% /s3
# cp -p /usr/bin/s3fs /s3
# ls -l /s3
total 4389
-rwx------ 1 root root 4493816 Dec 23 03:31 s3fs
#

